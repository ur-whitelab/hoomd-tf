{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. N-Body Neural Force Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable GPU. Remove this if you've compiled HOOMD for GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "# import the hoomd, htf packages\n",
    "import hoomd\n",
    "import hoomd.htf as htf\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from MDAnalysis import Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the SimModel\n",
    "\n",
    "Here we build a 2 hidden-layer N-body neural force fields. The inputs are the nearest N neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NlistNN(htf.SimModel):\n",
    "    def setup(self, dim, top_neighs):\n",
    "        self.dense1 = tf.keras.layers.Dense(dim)\n",
    "        self.dense2 = tf.keras.layers.Dense(dim)\n",
    "        self.last = tf.keras.layers.Dense(1)\n",
    "        self.top_neighs = top_neighs\n",
    "\n",
    "    def compute(self, nlist, positions, box):\n",
    "        rinv = htf.nlist_rinv(nlist)\n",
    "        # closest neighbors have largest value in 1/r, take top\n",
    "        sorted_n = tf.reshape(tf.sort(rinv, axis=1, direction='DESCENDING'), [-1, self.nneighbor_cutoff])\n",
    "        top_n = sorted_n[:, :self.top_neighs]\n",
    "        # run through NN\n",
    "        x = self.dense1(top_n)\n",
    "        x = self.dense2(x)\n",
    "        pair_energy = self.last(x)\n",
    "        # get per-particle energy\n",
    "        energy = tf.reduce_sum(pair_energy, axis=1)\n",
    "        forces = htf.compute_nlist_forces(nlist, energy)\n",
    "        # don't output last column of forces, pairwise energy, since it's meaningless here\n",
    "        return forces[:,:3], energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from a Trajectory File\n",
    "\n",
    "We will compile our model and then train against a trajectory. If this were a real example, we would be training against the forces in the trajectory but this trajectory (to save space) has no forces. Instead, we will train to make our forces match the positions of the trajctory, which makes no physical sense, but it has the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/801 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 366, 366, 3\n  y sizes: 366\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8933d2ed09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         dataset = tf.data.Dataset.from_tensors(\n\u001b[1;32m     12\u001b[0m                 (tuple(inputs), labels))\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/htf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[1;32m   1724\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m                                                     class_weight)\n\u001b[0m\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/htf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m   \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/htf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 366, 366, 3\n  y sizes: 366\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model = NlistNN(128, dim=16, top_neighs=8)\n",
    "# when we compile, add a None loss so as to not train the second output\n",
    "# which is energy\n",
    "model.compile('Adam', ['MeanSquaredError', None])\n",
    "universe = Universe('test_topol.pdb', 'test_traj.trr')\n",
    "losses = []\n",
    "for epoch in range(3):\n",
    "    for inputs, ts in htf.iter_from_trajectory(128, universe, r_cut=25, period=5):\n",
    "        #labels = ts.forces\n",
    "        labels = ts.positions\n",
    "        loss = model.train_on_batch(inputs, labels)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferrence with the Model\n",
    "\n",
    "Now we will run a particle simulation with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Hoomd-Sim Code ################\n",
    "hoomd.context.initialize('--mode=cpu')\n",
    "\n",
    "tfcompute = htf.tfcompute(model)\n",
    "\n",
    "# create a square lattice\n",
    "system = hoomd.init.create_lattice(unitcell=hoomd.lattice.sq(a=1.2),\n",
    "                                    n=[16,16])\n",
    "nlist = hoomd.md.nlist.cell()\n",
    "# NVT ensemble\n",
    "hoomd.md.integrate.mode_standard(dt=0.001)\n",
    "hoomd.md.integrate.nvt(group=hoomd.group.all(), kT=0.1, tau=0.5).randomize_velocities(seed=1)\n",
    "tfcompute.attach(nlist, r_cut=5, save_output_period=5)\n",
    "#run with our silly model\n",
    "hoomd.run(1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "This model is not properly trained so there is not much analysis to do. We'll just take a look at the energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = tf.reduce_sum(tfcompute.outputs[0], axis=1)\n",
    "plt.plot(e)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
